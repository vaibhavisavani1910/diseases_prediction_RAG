{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'fine_tuning_diseases.txt'\n",
    "output_dir = \"GPT-Neo-Disease-Symptoms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling, GPT2Tokenizer, GPTNeoForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare the dataset\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size,\n",
    "    )\n",
    "\n",
    "# Prepare the data collator\n",
    "def load_data_collator(tokenizer, mlm=False):\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm,\n",
    "    )\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = load_dataset(output_file, tokenizer)\n",
    "data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "# Load the GPT-Neo model\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 100/720 [01:13<07:26,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7697, 'grad_norm': 5.780911922454834, 'learning_rate': 4.305555555555556e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 200/720 [02:27<06:23,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.266, 'grad_norm': 5.402742862701416, 'learning_rate': 3.611111111111111e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 300/720 [03:39<04:58,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.97, 'grad_norm': 4.976509094238281, 'learning_rate': 2.916666666666667e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 400/720 [04:51<03:48,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6732, 'grad_norm': 5.329981803894043, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 500/720 [06:03<02:41,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5094, 'grad_norm': 4.8598127365112305, 'learning_rate': 1.527777777777778e-05, 'epoch': 3.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 600/720 [07:19<01:26,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3903, 'grad_norm': 5.066586494445801, 'learning_rate': 8.333333333333334e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 700/720 [08:31<00:14,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2923, 'grad_norm': 5.063773155212402, 'learning_rate': 1.388888888888889e-06, 'epoch': 4.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 720/720 [08:48<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 528.4542, 'train_samples_per_second': 5.421, 'train_steps_per_second': 1.362, 'train_loss': 1.8212705612182618, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('GPT-Neo-Disease-Symptoms/tokenizer_config.json',\n",
       " 'GPT-Neo-Disease-Symptoms/special_tokens_map.json',\n",
       " 'GPT-Neo-Disease-Symptoms/vocab.json',\n",
       " 'GPT-Neo-Disease-Symptoms/merges.txt',\n",
       " 'GPT-Neo-Disease-Symptoms/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5, \n",
    "    per_device_train_batch_size=4,  \n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=f'{output_dir}/logs',\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fine_tuning_diseases.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/8092brhd2wj7lx4kzrqzvxdm0000gp/T/ipykernel_55159/1594280234.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  Document(page_content=row[0])\n"
     ]
    }
   ],
   "source": [
    "# Convert the data to LangChain Documents\n",
    "documents = [\n",
    "    Document(page_content=row[0])\n",
    "    for _, row in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/8092brhd2wj7lx4kzrqzvxdm0000gp/T/ipykernel_55159/4219329043.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for the documents\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a FAISS index\n",
    "knowledge_base = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant context\n",
    "def retrieve_context(symptoms, k=1):\n",
    "    query = f\"Symptoms: {symptoms}\"\n",
    "    results = knowledge_base.similarity_search(query, k=k)\n",
    "    return \" \".join([doc.page_content for doc in results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "# Load the fine-tuned GPT-Neo model and tokenizer\n",
    "model_path = \"GPT-Neo-Disease-Symptoms\" \n",
    "model = GPTNeoForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_disease_with_rag(symptoms):\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(symptoms, k=1)\n",
    "    \n",
    "    # Create input text with retrieved context\n",
    "    input_text = (\n",
    "        \"You are a medical assistant trained to predict diseases based on symptoms.\\n\\n\"\n",
    "        f\"Context: {context}\\n\"\n",
    "        f\"Symptoms: {symptoms}\\n\\n\"\n",
    "        \"Based on the context and symptoms, provide the name of the disease. \"\n",
    "        \"If the context does not contain the answer, respond with: 'I don't know the answer.'\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize input with attention mask\n",
    "    inputs = tokenizer(\n",
    "        input_text[:tokenizer.model_max_length],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],  \n",
    "        max_length=min(len(inputs['input_ids'][0]) + 50, tokenizer.model_max_length),\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode the generated output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    #  Process the response to extract the disease name\n",
    "    response_lines = response.strip().split(\"\\n\")\n",
    "    for line in response_lines:\n",
    "        if \"this could be the disease:\" in line:\n",
    "            # Extract the disease name after the phrase\n",
    "            disease_name = line.split(\"this could be the disease:\")[-1].strip()\n",
    "            if disease_name:\n",
    "                # Remove any trailing period\n",
    "                disease_name = disease_name.rstrip(\".\")\n",
    "                return disease_name  # Return the extracted disease name\n",
    "\n",
    "    # Default response if no valid disease name is found\n",
    "    return \"I don't know the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swallowing problems\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_symptoms = \"I‚Äôm having trouble swallowing, and sometimes it feels like food is stuck in my throat. I‚Äôve also lost some weight. What might be going on?\"\n",
    "diagnosis = identify_disease_with_rag(user_symptoms)\n",
    "print(diagnosis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atrial fibrillation\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_symptoms = \"fgerhb wih fiwehr iwuehr iewr\"\n",
    "diagnosis = identify_disease_with_rag(user_symptoms)\n",
    "print(diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.26%\n",
      "\n",
      "Mismatched cases:\n",
      "Symptoms: I have deep, constant pain in my belly and back, and I feel a pulse near my bellybutton. What could this be?\n",
      "Actual Disease: Abdominal aortic aneurysm\n",
      "Predicted Disease: AAA\n",
      "--------------------------------------------------\n",
      "Symptoms: I‚Äôm having trouble swallowing, and sometimes it feels like food is stuck in my throat. I‚Äôve also lost some weight. What might be going on?\n",
      "Actual Disease: Achalasia\n",
      "Predicted Disease: Swallowing problems\n",
      "--------------------------------------------------\n",
      "Symptoms: I feel severe pain in my upper right belly that spreads to my shoulder, and I‚Äôve been feeling nauseous and feverish. What could this mean?\n",
      "Actual Disease: Acute cholecystitis\n",
      "Predicted Disease: Pancreatitis (acute)\n",
      "--------------------------------------------------\n",
      "Symptoms: I‚Äôve been losing hearing on one side, and there‚Äôs ringing in my ear. Sometimes I feel dizzy and off balance. What might this be?\n",
      "Actual Disease: Acoustic neuroma (vestibular schwannoma)\n",
      "Predicted Disease: M√©ni√®re's disease\n",
      "--------------------------------------------------\n",
      "Symptoms: I‚Äôm always tired, losing weight, and I‚Äôve noticed dark patches on my skin. What could this indicate?\n",
      "Actual Disease: Addison's disease\n",
      "Predicted Disease: Sunburn\n",
      "--------------------------------------------------\n",
      "Symptoms: I have severe back pain that spreads down my legs and causes numbness and tingling. What could it be?\n",
      "Actual Disease: Back pain due to nerve compression or another serious cause\n",
      "Predicted Disease: Upper back pain\n",
      "--------------------------------------------------\n",
      "Symptoms: My breath smells bad even though I brush regularly, and I‚Äôve noticed a dry mouth and gum issues. What could be causing this?\n",
      "Actual Disease: Bad breath caused by dry mouth, gum disease, or other oral conditions\n",
      "Predicted Disease: \"Bad breath caused by dry mouth, gum disease, or other oral conditions.\"\n",
      "--------------------------------------------------\n",
      "Symptoms: I have redness and small swollen bumps on my face that look like acne, and sometimes my skin feels hot and tender. What could this be?\n",
      "Actual Disease: Rosacea\n",
      "Predicted Disease: Sunburn\n",
      "--------------------------------------------------\n",
      "Symptoms: I feel a persistent dull pain in my jaw, along with difficulty chewing and tenderness around my ear. What condition might I have?\n",
      "Actual Disease: Temporomandibular disorder (TMD)\n",
      "Predicted Disease: Jaw pain\n",
      "--------------------------------------------------\n",
      "Symptoms: I‚Äôve noticed a small lump in my scrotum, and there‚Äôs a dull ache in my lower belly. Should I be worried about something serious?\n",
      "Actual Disease: Testicular cancer\n",
      "Predicted Disease: Pain in testicles\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test dataset\n",
    "test_data_path = \"symptom_diseases_test.csv\"\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(test_df):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(test_df)\n",
    "    mismatched_cases = []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        symptoms = row[\"symptoms\"]\n",
    "        actual_disease = row[\"disease\"]\n",
    "        actual_disease = actual_disease.rstrip(\".\")\n",
    "        predicted_disease = identify_disease_with_rag(symptoms)\n",
    "        \n",
    "        if predicted_disease.lower() == actual_disease.lower():\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            mismatched_cases.append(\n",
    "                {\n",
    "                    \"Symptoms\": symptoms,\n",
    "                    \"Actual Disease\": actual_disease,\n",
    "                    \"Predicted Disease\": predicted_disease,\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy, mismatched_cases\n",
    "\n",
    "# Run the evaluation\n",
    "accuracy, mismatched_cases = evaluate_model(test_df)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "if mismatched_cases:\n",
    "    print(\"\\nMismatched cases:\")\n",
    "    for case in mismatched_cases:\n",
    "        print(f\"Symptoms: {case['Symptoms']}\")\n",
    "        print(f\"Actual Disease: {case['Actual Disease']}\")\n",
    "        print(f\"Predicted Disease: {case['Predicted Disease']}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `30` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `70` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "Top K: 10\n",
      "Temperature: 0.7\n",
      "Max Length Offset: 20\n",
      "Best Accuracy: 78.26%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Hyperparameter Tuning Class\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def tune_generation_parameters(self, test_df):\n",
    "        # Define hyperparameter grid\n",
    "        param_grid = {\n",
    "            'top_k': [10, 30, 50, 70],\n",
    "            'temperature': [0.7, 0.9, 1.1],\n",
    "            'max_length_offset': [20, 50, 100]\n",
    "        }\n",
    "        \n",
    "        best_accuracy = 0\n",
    "        best_params = {}\n",
    "        \n",
    "        for top_k in param_grid['top_k']:\n",
    "            for temperature in param_grid['temperature']:\n",
    "                for max_length_offset in param_grid['max_length_offset']:\n",
    "                    accuracy = self._evaluate_params(\n",
    "                        test_df, \n",
    "                        top_k=top_k, \n",
    "                        temperature=temperature, \n",
    "                        max_length_offset=max_length_offset\n",
    "                    )\n",
    "                    \n",
    "                    if accuracy > best_accuracy:\n",
    "                        best_accuracy = accuracy\n",
    "                        best_params = {\n",
    "                            'top_k': top_k,\n",
    "                            'temperature': temperature,\n",
    "                            'max_length_offset': max_length_offset\n",
    "                        }\n",
    "        \n",
    "        print(\"Best Hyperparameters:\")\n",
    "        print(f\"Top K: {best_params['top_k']}\")\n",
    "        print(f\"Temperature: {best_params['temperature']}\")\n",
    "        print(f\"Max Length Offset: {best_params['max_length_offset']}\")\n",
    "        print(f\"Best Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "    def _evaluate_params(self, test_df, top_k, temperature, max_length_offset):\n",
    "        correct_predictions = 0\n",
    "        total_predictions = len(test_df)\n",
    "        \n",
    "        for _, row in test_df.iterrows():\n",
    "            symptoms = row[\"symptoms\"]\n",
    "            actual_disease = row[\"disease\"].rstrip('.')  # Remove trailing period\n",
    "            \n",
    "            # Retrieve context\n",
    "            context = retrieve_context(symptoms, k=1)\n",
    "            \n",
    "            # Create input text\n",
    "            input_text = (\n",
    "                \"You are a medical assistant trained to predict diseases based on symptoms.\\n\\n\"\n",
    "                f\"Context: {context}\\n\"\n",
    "                f\"Symptoms: {symptoms}\\n\\n\"\n",
    "                \"Based on the context and symptoms, provide the name of the disease. \"\n",
    "                \"If the context does not contain the answer, respond with: 'I don't know the answer.'\"\n",
    "            )\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                input_text[:self.tokenizer.model_max_length],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Generate response with tuned parameters\n",
    "            outputs = self.model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],  \n",
    "                max_length=min(len(inputs['input_ids'][0]) + max_length_offset, self.tokenizer.model_max_length),\n",
    "                top_k=top_k,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            # Decode and process response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response_lines = response.strip().split(\"\\n\")\n",
    "            \n",
    "            predicted_disease = None\n",
    "            for line in response_lines:\n",
    "                if \"this could be the disease:\" in line:\n",
    "                    predicted_disease = line.split(\"this could be the disease:\")[-1].strip().rstrip('.')\n",
    "                    break\n",
    "            \n",
    "            if predicted_disease and predicted_disease.lower() == actual_disease.lower():\n",
    "                correct_predictions += 1\n",
    "        \n",
    "        return correct_predictions / total_predictions\n",
    "\n",
    "# Load the test dataset\n",
    "test_data_path = \"symptom_diseases_test.csv\"\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "hf_token = \"\"\n",
    "model_path = \"GPT-Neo-Disease-Symptoms\" \n",
    "model = GPTNeoForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "tuner = HyperparameterTuner(model, tokenizer)\n",
    "best_hyperparams = tuner.tune_generation_parameters(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
